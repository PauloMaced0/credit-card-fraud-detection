{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection using Machine Learning\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook develops a machine learning model to detect and prevent credit card fraud by identifying suspicious transactions. Machine learning provides an effective approach to automatically identify fraudulent transactions based on patterns in the data.\n",
    "\n",
    "### What is Anomaly Detection?\n",
    "\n",
    "**Anomaly detection** (also called outlier detection) is a technique used to identify rare items, events, or observations that deviate significantly from the majority of the data. In the context of credit card fraud:\n",
    "\n",
    "- **Normal transactions** represent the majority of data (legitimate purchases)\n",
    "- **Anomalies/Outliers** represent fraudulent transactions that exhibit unusual patterns\n",
    "\n",
    "There are two main approaches to anomaly detection:\n",
    "1. **Supervised Learning**: When we have labeled data (fraud/not fraud), we can train classifiers\n",
    "2. **Unsupervised Learning**: When labels are unavailable, we identify anomalies based on statistical properties\n",
    "\n",
    "Since our dataset is **labeled** (has a `Class` column), we'll use **supervised classification** while also exploring some unsupervised methods for comparison.\n",
    "\n",
    "### Dataset Features\n",
    "\n",
    "- `id`: Unique identifier for each transaction\n",
    "- `V1-V28`: Anonymized features (result of PCA transformation for privacy)\n",
    "- `Amount`: Transaction amount\n",
    "- `Class`: Target variable (0 = legitimate, 1 = fraudulent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We'll import all necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and numerical operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics and Evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration\n",
    "\n",
    "Let's load the dataset and perform initial exploration to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# IMPORTANT: Update this path to your actual dataset location\n",
    "df = pd.read_csv('creditcard_2023.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and non-null counts\n",
    "print(\"\\nData Types and Non-Null Counts:\")\n",
    "print(\"-\"*40)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n",
    "\n",
    "Before building our model, we need to check for:\n",
    "- Missing values\n",
    "- Duplicate records\n",
    "- Data types consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "print(\"\\n1. MISSING VALUES ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "if missing_values.sum() == 0:\n",
    "    print(\" No missing values found in the dataset!\")\n",
    "else:\n",
    "    print(\"Missing values found:\")\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_values[missing_values > 0],\n",
    "        'Percentage': missing_percentage[missing_values > 0]\n",
    "    })\n",
    "    print(missing_df)\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\n2. DUPLICATE RECORDS\")\n",
    "print(\"-\"*40)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates:,}\")\n",
    "print(f\"Percentage of duplicates: {(duplicates/len(df))*100:.2f}%\")\n",
    "\n",
    "# Check unique IDs\n",
    "print(\"\\n3. ID UNIQUENESS\")\n",
    "print(\"-\"*40)\n",
    "unique_ids = df['id'].nunique()\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique IDs: {unique_ids:,}\")\n",
    "print(f\"IDs are unique: {unique_ids == len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Class Distribution Analysis (CRITICAL for Fraud Detection)\n",
    "\n",
    "### Why Class Balance Matters\n",
    "\n",
    "In fraud detection, class imbalance is a critical concern:\n",
    "\n",
    "- **Imbalanced datasets** (typical in real-world fraud): ~99% legitimate, ~1% fraudulent\n",
    "- **Balanced datasets**: Roughly equal distribution between classes\n",
    "\n",
    "**Impact of Imbalance:**\n",
    "- Models may become biased toward the majority class\n",
    "- High accuracy can be misleading (predicting all as legitimate gives 99% accuracy but catches 0 frauds)\n",
    "- Special techniques needed: oversampling, undersampling, cost-sensitive learning\n",
    "\n",
    "Let's analyze our dataset's class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = df['Class'].value_counts()\n",
    "class_percentages = df['Class'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Legitimate Transactions (Class 0): {class_counts[0]:,} ({class_percentages[0]:.2f}%)\")\n",
    "print(f\"Fraudulent Transactions (Class 1): {class_counts[1]:,} ({class_percentages[1]:.2f}%)\")\n",
    "print(f\"\\nClass Ratio (Legitimate:Fraudulent): {class_counts[0]/class_counts[1]:.2f}:1\")\n",
    "\n",
    "# Determine if balanced\n",
    "imbalance_ratio = class_counts[0] / class_counts[1]\n",
    "if 0.8 <= class_percentages[1]/class_percentages[0] <= 1.2:\n",
    "    balance_status = \"BALANCED\"\n",
    "    balance_note = \"\"\"The dataset is balanced, meaning both classes have similar representation.\n",
    "This simplifies our modeling approach as we don't need to apply special techniques\n",
    "like SMOTE (Synthetic Minority Over-sampling Technique) or class weights.\"\"\"\n",
    "else:\n",
    "    balance_status = \"IMBALANCED\"\n",
    "    balance_note = \"The dataset is imbalanced. Consider using techniques like SMOTE or class weights.\"\n",
    "\n",
    "print(f\"\\nDataset Status: {balance_status}\")\n",
    "print(f\"\\nNote: {balance_note}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = axes[0].bar(['Legitimate (0)', 'Fraudulent (1)'], class_counts.values, color=colors, edgecolor='black')\n",
    "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Transactions', fontsize=12)\n",
    "axes[0].set_xlabel('Transaction Class', fontsize=12)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
    "                 f'{count:,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(class_counts.values, labels=['Legitimate', 'Fraudulent'], \n",
    "            autopct='%1.1f%%', colors=colors, explode=(0, 0.05),\n",
    "            shadow=True, startangle=90)\n",
    "axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis\n",
    "\n",
    "### Understanding the Features\n",
    "\n",
    "The features V1-V28 are the result of **Principal Component Analysis (PCA)** transformation. This was done to:\n",
    "1. **Protect sensitive information** (anonymization)\n",
    "2. **Reduce dimensionality** while preserving variance\n",
    "3. **Remove multicollinearity** between original features\n",
    "\n",
    "The `Amount` feature was NOT transformed and represents the actual transaction amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Amount feature\n",
    "print(\"=\"*60)\n",
    "print(\"AMOUNT FEATURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nAmount Statistics by Class:\")\n",
    "print(\"-\"*40)\n",
    "amount_stats = df.groupby('Class')['Amount'].describe()\n",
    "print(amount_stats)\n",
    "\n",
    "# Compare mean and median\n",
    "print(\"\\n\\nKey Observations:\")\n",
    "print(\"-\"*40)\n",
    "legit_mean = df[df['Class']==0]['Amount'].mean()\n",
    "fraud_mean = df[df['Class']==1]['Amount'].mean()\n",
    "legit_median = df[df['Class']==0]['Amount'].median()\n",
    "fraud_median = df[df['Class']==1]['Amount'].median()\n",
    "\n",
    "print(f\"Legitimate - Mean: ${legit_mean:.2f}, Median: ${legit_median:.2f}\")\n",
    "print(f\"Fraudulent - Mean: ${fraud_mean:.2f}, Median: ${fraud_median:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Amount distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution by class\n",
    "for cls, color, label in [(0, '#2ecc71', 'Legitimate'), (1, '#e74c3c', 'Fraudulent')]:\n",
    "    subset = df[df['Class'] == cls]['Amount']\n",
    "    axes[0].hist(subset, bins=50, alpha=0.6, color=color, label=label, density=True)\n",
    "\n",
    "axes[0].set_title('Transaction Amount Distribution by Class', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Amount ($)', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, df['Amount'].quantile(0.99))  # Limit to 99th percentile for better visualization\n",
    "\n",
    "# Box plot\n",
    "df.boxplot(column='Amount', by='Class', ax=axes[1])\n",
    "axes[1].set_title('Amount Box Plot by Class', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Class (0=Legitimate, 1=Fraudulent)', fontsize=12)\n",
    "axes[1].set_ylabel('Amount ($)', fontsize=12)\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('amount_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze V features - Distribution comparison\n",
    "v_features = [f'V{i}' for i in range(1, 29)]\n",
    "\n",
    "# Calculate mean differences between classes\n",
    "mean_diff = []\n",
    "for feature in v_features:\n",
    "    fraud_mean = df[df['Class']==1][feature].mean()\n",
    "    legit_mean = df[df['Class']==0][feature].mean()\n",
    "    mean_diff.append(abs(fraud_mean - legit_mean))\n",
    "\n",
    "# Sort features by discriminative power\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': v_features,\n",
    "    'Mean_Difference': mean_diff\n",
    "}).sort_values('Mean_Difference', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TOP DISCRIMINATIVE FEATURES (by mean difference)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFeatures with largest mean difference between classes:\")\n",
    "print(\"(These features may be most useful for distinguishing fraud)\")\n",
    "print(\"-\"*40)\n",
    "print(feature_importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top discriminative features\n",
    "top_features = feature_importance_df['Feature'].head(6).tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    for cls, color, label in [(0, '#2ecc71', 'Legitimate'), (1, '#e74c3c', 'Fraudulent')]:\n",
    "        subset = df[df['Class'] == cls][feature]\n",
    "        axes[idx].hist(subset, bins=50, alpha=0.6, color=color, label=label, density=True)\n",
    "    \n",
    "    axes[idx].set_title(f'{feature} Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.suptitle('Top 6 Most Discriminative Features', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_features_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with target\n",
    "correlations = df.drop('id', axis=1).corr()['Class'].drop('Class').sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CORRELATION WITH TARGET (Class)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTop 10 features most correlated with fraud:\")\n",
    "print(\"-\"*40)\n",
    "print(correlations.head(10))\n",
    "\n",
    "print(\"\\n\\nBottom 10 features least correlated with fraud:\")\n",
    "print(\"-\"*40)\n",
    "print(correlations.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations with target\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in correlations.values]\n",
    "plt.barh(range(len(correlations)), correlations.values, color=colors)\n",
    "plt.yticks(range(len(correlations)), correlations.index)\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.title('Feature Correlations with Fraud (Class)', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_with_target.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Preprocessing\n",
    "\n",
    "### Preprocessing Steps:\n",
    "\n",
    "1. **Remove non-predictive features**: The `id` column is just an identifier\n",
    "2. **Feature Scaling**: Important for algorithms like Logistic Regression\n",
    "   - V1-V28 are already scaled (from PCA)\n",
    "   - `Amount` needs scaling\n",
    "\n",
    "### Why Scale Features?\n",
    "\n",
    "- **Gradient-based algorithms** (Logistic Regression, Neural Networks) converge faster\n",
    "- **Distance-based algorithms** (KNN, SVM) are sensitive to feature magnitudes\n",
    "- **Tree-based algorithms** (Random Forest, XGBoost) are scale-invariant but scaling doesn't hurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Remove the id column\n",
    "print(\"\\n1. Removing non-predictive features...\")\n",
    "print(\"    Dropping 'id' column\")\n",
    "df_processed = df_processed.drop('id', axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop('Class', axis=1)\n",
    "y = df_processed['Class']\n",
    "\n",
    "print(f\"\\n2. Feature matrix shape: {X.shape}\")\n",
    "print(f\"   Target vector shape: {y.shape}\")\n",
    "\n",
    "# Scale the Amount feature using RobustScaler\n",
    "# RobustScaler is better for data with outliers as it uses median and IQR\n",
    "print(\"\\n3. Scaling 'Amount' feature using RobustScaler...\")\n",
    "print(\"    RobustScaler chosen because Amount has outliers\")\n",
    "print(\"    Uses median and IQR instead of mean and std\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X['Amount'] = scaler.fit_transform(X[['Amount']])\n",
    "\n",
    "print(\"\\n Preprocessing complete!\")\n",
    "print(f\"\\nFinal feature statistics:\")\n",
    "print(X.describe().T[['mean', 'std', 'min', 'max']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train-Test Split\n",
    "\n",
    "### Why Split the Data?\n",
    "\n",
    "We need to evaluate our model on data it has never seen during training. This gives us an unbiased estimate of real-world performance.\n",
    "\n",
    "### Split Strategy:\n",
    "\n",
    "- **Training Set (80%)**: Used to train the model\n",
    "- **Test Set (20%)**: Used for final evaluation (held out, never used during training)\n",
    "\n",
    "### Important: Stratified Splitting\n",
    "\n",
    "We use **stratified splitting** to ensure both training and test sets have the same class distribution as the original dataset. This is crucial for classification problems to ensure representative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Perform stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.20,           # 20% for testing\n",
    "    random_state=RANDOM_STATE, # For reproducibility\n",
    "    stratify=y                 # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(\"\\nSplit Configuration:\")\n",
    "print(\"-\"*40)\n",
    "print(f\" Test size: 20%\")\n",
    "print(f\" Stratified: Yes (maintains class proportions)\")\n",
    "print(f\" Random state: {RANDOM_STATE} (for reproducibility)\")\n",
    "\n",
    "print(\"\\n\\nDataset Sizes:\")\n",
    "print(\"-\"*40)\n",
    "print(f\" Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\" Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\n\\nClass Distribution Verification:\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Original - Class 0: {(y==0).sum()/len(y)*100:.2f}%, Class 1: {(y==1).sum()/len(y)*100:.2f}%\")\n",
    "print(f\"Training - Class 0: {(y_train==0).sum()/len(y_train)*100:.2f}%, Class 1: {(y_train==1).sum()/len(y_train)*100:.2f}%\")\n",
    "print(f\"Test     - Class 0: {(y_test==0).sum()/len(y_test)*100:.2f}%, Class 1: {(y_test==1).sum()/len(y_test)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n Stratification maintained class proportions in both sets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Selection and Training\n",
    "\n",
    "### Models to Evaluate:\n",
    "\n",
    "1. **Logistic Regression**: Simple, interpretable baseline\n",
    "2. **Decision Tree**: Non-linear, easy to interpret\n",
    "3. **Random Forest**: Ensemble of trees, robust to overfitting\n",
    "4. **XGBoost**: Gradient boosting, often best performance\n",
    "\n",
    "### Why These Models?\n",
    "\n",
    "| Model | Pros | Cons |\n",
    "|-------|------|------|\n",
    "| Logistic Regression | Fast, interpretable, good baseline | Limited to linear relationships |\n",
    "| Decision Tree | Handles non-linearity, interpretable | Prone to overfitting |\n",
    "| Random Forest | Reduces overfitting, handles complex patterns | Less interpretable, slower |\n",
    "| XGBoost | State-of-the-art performance, handles imbalance | Complex to tune, less interpretable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=1000,\n",
    "        solver='lbfgs'\n",
    "    ),\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_depth=10,  # Limit depth to prevent overfitting\n",
    "        min_samples_split=10\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        n_jobs=-1  # Use all CPU cores\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train all models and store results\n",
    "trained_models = {}\n",
    "training_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Store results\n",
    "    training_results[name] = {\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    print(f\"    {name} trained successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All models trained successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation Metrics\n",
    "\n",
    "### Understanding the Metrics\n",
    "\n",
    "For fraud detection, **accuracy alone is NOT sufficient**. Here's why each metric matters:\n",
    "\n",
    "| Metric | Formula | What it Measures | Why it Matters for Fraud |\n",
    "|--------|---------|------------------|-------------------------|\n",
    "| **Accuracy** | (TP+TN)/(Total) | Overall correctness | Can be misleading with imbalanced data |\n",
    "| **Precision** | TP/(TP+FP) | Of predicted frauds, how many are real? | High FP = legitimate transactions blocked |\n",
    "| **Recall** | TP/(TP+FN) | Of actual frauds, how many caught? | High FN = frauds go undetected (costly!) |\n",
    "| **F1 Score** | 2×(P×R)/(P+R) | Balance of Precision & Recall | Overall fraud detection effectiveness |\n",
    "| **ROC-AUC** | Area under ROC curve | Ability to distinguish classes | Performance across all thresholds |\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **True Positive (TP)**: Correctly identified fraud\n",
    "- **True Negative (TN)**: Correctly identified legitimate transaction\n",
    "- **False Positive (FP)**: Legitimate transaction flagged as fraud (customer inconvenience)\n",
    "- **False Negative (FN)**: Fraud not detected (financial loss!)\n",
    "\n",
    "### For Fraud Detection:\n",
    "- **Recall is often most important** because missing a fraud (FN) is typically more costly than a false alarm (FP)\n",
    "- However, very low precision creates too many false alarms, frustrating legitimate customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON - EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(training_results.keys()),\n",
    "    'Accuracy': [training_results[m]['accuracy'] for m in training_results],\n",
    "    'Precision': [training_results[m]['precision'] for m in training_results],\n",
    "    'Recall': [training_results[m]['recall'] for m in training_results],\n",
    "    'F1 Score': [training_results[m]['f1'] for m in training_results],\n",
    "    'ROC-AUC': [training_results[m]['roc_auc'] for m in training_results]\n",
    "})\n",
    "\n",
    "# Format as percentages\n",
    "comparison_display = comparison_df.copy()\n",
    "for col in ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']:\n",
    "    comparison_display[col] = comparison_display[col].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(comparison_display.to_string(index=False))\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\n\\nBest Model by Metric:\")\n",
    "print(\"-\"*40)\n",
    "for col in ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']:\n",
    "    best_idx = comparison_df[col].idxmax()\n",
    "    best_model = comparison_df.loc[best_idx, 'Model']\n",
    "    best_value = comparison_df.loc[best_idx, col]\n",
    "    print(f\"{col}: {best_model} ({best_value*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "multiplier = 0\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "for idx, (model_name, results) in enumerate(training_results.items()):\n",
    "    values = [results['accuracy'], results['precision'], results['recall'], \n",
    "              results['f1'], results['roc_auc']]\n",
    "    offset = width * multiplier\n",
    "    bars = ax.bar(x + offset, values, width, label=model_name, color=colors[idx])\n",
    "    multiplier += 1\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(metrics, fontsize=11)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrix Analysis\n",
    "\n",
    "### What is a Confusion Matrix?\n",
    "\n",
    "A confusion matrix is a table that visualizes the performance of a classification model by showing the counts of:\n",
    "- **True Positives (TP)**: Correctly predicted frauds\n",
    "- **True Negatives (TN)**: Correctly predicted legitimate transactions\n",
    "- **False Positives (FP)**: Legitimate transactions incorrectly flagged as fraud\n",
    "- **False Negatives (FN)**: Frauds that were not detected\n",
    "\n",
    "```\n",
    "                    Predicted\n",
    "                 Negative  Positive\n",
    "Actual Negative    TN        FP\n",
    "Actual Positive    FN        TP\n",
    "```\n",
    "\n",
    "### Business Impact:\n",
    "- **FP (False Alarms)**: Customer inconvenience, declined legitimate purchases\n",
    "- **FN (Missed Frauds)**: Financial losses, customer trust damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, results) in enumerate(training_results.items()):\n",
    "    cm = confusion_matrix(y_test, results['y_pred'])\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create annotations with both count and percentage\n",
    "    annotations = np.array([[f'{cm[i,j]:,}\\n({cm_percent[i,j]:.1f}%)' \n",
    "                            for j in range(2)] for i in range(2)])\n",
    "    \n",
    "    sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Legitimate', 'Fraud'],\n",
    "                yticklabels=['Legitimate', 'Fraud'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    axes[idx].set_title(f'{model_name}\\nAccuracy: {results[\"accuracy\"]*100:.2f}%', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11)\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11)\n",
    "\n",
    "plt.suptitle('Confusion Matrices - All Models', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed confusion matrix analysis\n",
    "print(\"=\"*60)\n",
    "print(\"DETAILED CONFUSION MATRIX ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, results in training_results.items():\n",
    "    cm = confusion_matrix(y_test, results['y_pred'])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"  True Negatives (TN):  {tn:,} - Correctly identified legitimate\")\n",
    "    print(f\"  False Positives (FP): {fp:,} - Legitimate flagged as fraud (false alarm)\")\n",
    "    print(f\"  False Negatives (FN): {fn:,} - Fraud not detected (MISSED!)\")\n",
    "    print(f\"  True Positives (TP):  {tp:,} - Correctly identified fraud\")\n",
    "    print(f\"\\n  Detection Rate: {tp/(tp+fn)*100:.2f}% of frauds caught\")\n",
    "    print(f\"  False Alarm Rate: {fp/(fp+tn)*100:.2f}% of legitimate flagged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ROC Curve and AUC Analysis\n",
    "\n",
    "### What is ROC-AUC?\n",
    "\n",
    "**ROC (Receiver Operating Characteristic)** curve plots:\n",
    "- **True Positive Rate (Sensitivity/Recall)** vs **False Positive Rate (1 - Specificity)**\n",
    "- At various classification thresholds\n",
    "\n",
    "**AUC (Area Under Curve)**:\n",
    "- Ranges from 0 to 1\n",
    "- 0.5 = Random guessing (diagonal line)\n",
    "- 1.0 = Perfect classifier\n",
    "- Higher is better\n",
    "\n",
    "### Interpretation:\n",
    "- **AUC > 0.9**: Excellent\n",
    "- **AUC 0.8-0.9**: Good\n",
    "- **AUC 0.7-0.8**: Fair\n",
    "- **AUC < 0.7**: Poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "# ROC Curve\n",
    "for idx, (model_name, results) in enumerate(training_results.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test, results['y_pred_proba'])\n",
    "    auc = roc_auc_score(y_test, results['y_pred_proba'])\n",
    "    axes[0].plot(fpr, tpr, color=colors[idx], lw=2, \n",
    "                 label=f'{model_name} (AUC = {auc:.4f})')\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "axes[0].set_xlim([0.0, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='lower right', fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "for idx, (model_name, results) in enumerate(training_results.items()):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, results['y_pred_proba'])\n",
    "    ap = average_precision_score(y_test, results['y_pred_proba'])\n",
    "    axes[1].plot(recall, precision, color=colors[idx], lw=2, \n",
    "                 label=f'{model_name} (AP = {ap:.4f})')\n",
    "\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='lower left', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_pr_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cross-Validation\n",
    "\n",
    "### Why Cross-Validation?\n",
    "\n",
    "A single train-test split might give results that depend on how the data was split. **Cross-validation** provides a more robust estimate by:\n",
    "\n",
    "1. Splitting data into K folds\n",
    "2. Training K times, each time using a different fold as validation\n",
    "3. Averaging results across all folds\n",
    "\n",
    "### Stratified K-Fold:\n",
    "We use **Stratified** K-Fold to maintain class proportions in each fold, which is especially important for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CROSS-VALIDATION (5-Fold Stratified)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Perform 5-fold stratified cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "print(\"\\nPerforming cross-validation for each model...\")\n",
    "print(\"(This may take a moment)\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Cross-validating {name}...\")\n",
    "    \n",
    "    # Calculate CV scores for multiple metrics\n",
    "    accuracy_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(model, X, y, cv=cv, scoring='precision')\n",
    "    recall_scores = cross_val_score(model, X, y, cv=cv, scoring='recall')\n",
    "    f1_scores = cross_val_score(model, X, y, cv=cv, scoring='f1')\n",
    "    roc_auc_scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'Accuracy': (accuracy_scores.mean(), accuracy_scores.std()),\n",
    "        'Precision': (precision_scores.mean(), precision_scores.std()),\n",
    "        'Recall': (recall_scores.mean(), recall_scores.std()),\n",
    "        'F1 Score': (f1_scores.mean(), f1_scores.std()),\n",
    "        'ROC-AUC': (roc_auc_scores.mean(), roc_auc_scores.std())\n",
    "    }\n",
    "\n",
    "print(\"\\n Cross-validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display CV results\n",
    "print(\"\\nCross-Validation Results (Mean ± Std):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, results in cv_results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\"*50)\n",
    "    for metric, (mean, std) in results.items():\n",
    "        print(f\"  {metric:12s}: {mean*100:.2f}% (± {std*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"-\"*50)\n",
    "print(\" Low standard deviation indicates consistent performance across folds\")\n",
    "print(\" High standard deviation suggests the model is sensitive to data splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Feature Importance Analysis\n",
    "\n",
    "Understanding which features are most important helps us:\n",
    "1. Interpret the model's decisions\n",
    "2. Potentially reduce features for simpler models\n",
    "3. Gain business insights about fraud patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from tree-based models\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': trained_models['Random Forest'].feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# XGBoost feature importance\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': trained_models['XGBoost'].feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Features - Random Forest:\")\n",
    "print(\"-\"*40)\n",
    "print(rf_importance.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nTop 10 Features - XGBoost:\")\n",
    "print(\"-\"*40)\n",
    "print(xgb_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Random Forest\n",
    "top_rf = rf_importance.head(15)\n",
    "axes[0].barh(range(len(top_rf)), top_rf['Importance'].values, color='#3498db')\n",
    "axes[0].set_yticks(range(len(top_rf)))\n",
    "axes[0].set_yticklabels(top_rf['Feature'].values)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Importance', fontsize=12)\n",
    "axes[0].set_title('Random Forest - Feature Importance', fontsize=14, fontweight='bold')\n",
    "\n",
    "# XGBoost\n",
    "top_xgb = xgb_importance.head(15)\n",
    "axes[1].barh(range(len(top_xgb)), top_xgb['Importance'].values, color='#9b59b6')\n",
    "axes[1].set_yticks(range(len(top_xgb)))\n",
    "axes[1].set_yticklabels(top_xgb['Feature'].values)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('Importance', fontsize=12)\n",
    "axes[1].set_title('XGBoost - Feature Importance', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Unsupervised Anomaly Detection\n",
    "\n",
    "For comparison, let's also try **Isolation Forest**, an unsupervised anomaly detection algorithm.\n",
    "\n",
    "### How Isolation Forest Works:\n",
    "\n",
    "1. Builds random trees by randomly selecting features and split values\n",
    "2. Anomalies are isolated earlier (fewer splits needed) because they are different from normal points\n",
    "3. Normal points require more splits to isolate\n",
    "\n",
    "This approach is useful when labeled data is scarce or unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"UNSUPERVISED ANOMALY DETECTION - Isolation Forest\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train Isolation Forest\n",
    "# contamination is set based on our knowledge of fraud ratio\n",
    "fraud_ratio = (y == 1).sum() / len(y)\n",
    "print(f\"\\nSetting contamination parameter to: {fraud_ratio:.4f}\")\n",
    "print(\"(This tells the model the expected proportion of anomalies)\")\n",
    "\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=fraud_ratio,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "iso_forest.fit(X_train)\n",
    "y_pred_iso = iso_forest.predict(X_test)\n",
    "\n",
    "# Convert predictions: Isolation Forest returns -1 for anomalies, 1 for normal\n",
    "y_pred_iso = np.where(y_pred_iso == -1, 1, 0)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nIsolation Forest Results:\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_iso)*100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_iso)*100:.2f}%\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_iso)*100:.2f}%\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_iso)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nNote: Isolation Forest is unsupervised, so it doesn't use labels during training.\")\n",
    "print(\"Performance is lower than supervised methods but useful when labels are unavailable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Isolation Forest\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ISOLATION FOREST - CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cm_iso = confusion_matrix(y_test, y_pred_iso)\n",
    "tn_iso, fp_iso, fn_iso, tp_iso = cm_iso.ravel()\n",
    "\n",
    "# Calculate percentages\n",
    "cm_iso_percent = cm_iso.astype('float') / cm_iso.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create annotations with both count and percentage\n",
    "annotations_iso = np.array([[f'TN\\n{cm_iso[0,0]:,}\\n({cm_iso_percent[0,0]:.1f}%)', \n",
    "                             f'FP\\n{cm_iso[0,1]:,}\\n({cm_iso_percent[0,1]:.1f}%)'],\n",
    "                            [f'FN\\n{cm_iso[1,0]:,}\\n({cm_iso_percent[1,0]:.1f}%)', \n",
    "                             f'TP\\n{cm_iso[1,1]:,}\\n({cm_iso_percent[1,1]:.1f}%)']])\n",
    "\n",
    "sns.heatmap(cm_iso, annot=annotations_iso, fmt='', cmap='Oranges',\n",
    "            xticklabels=['Predicted\\nLegitimate', 'Predicted\\nFraudulent'],\n",
    "            yticklabels=['Actual\\nLegitimate', 'Actual\\nFraudulent'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "\n",
    "plt.title('Confusion Matrix - Isolation Forest (Unsupervised)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('isolation_forest_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Detailed analysis\n",
    "print(\"\\nIsolation Forest - Confusion Matrix Analysis:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"  True Negatives (TN):  {tn_iso:,} - Correctly identified legitimate\")\n",
    "print(f\"  False Positives (FP): {fp_iso:,} - Legitimate flagged as fraud\")\n",
    "print(f\"  False Negatives (FN): {fn_iso:,} - Fraud not detected (MISSED!)\")\n",
    "print(f\"  True Positives (TP):  {tp_iso:,} - Correctly identified fraud\")\n",
    "print(f\"\\n  Detection Rate: {tp_iso/(tp_iso+fn_iso)*100:.2f}% of frauds caught\")\n",
    "print(f\"  False Alarm Rate: {fp_iso/(fp_iso+tn_iso)*100:.2f}% of legitimate flagged\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Comparison Note:\")\n",
    "print(\"Isolation Forest is unsupervised - it detects anomalies based on\")\n",
    "print(\"how easily points can be 'isolated' in random trees, without using class labels.\")\n",
    "print(\"This makes it valuable when labeled fraud data is scarce or unavailable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Best Model Selection and Final Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find best model based on F1 Score (balanced metric for fraud detection)\n",
    "best_model_name = max(training_results, key=lambda x: training_results[x]['f1'])\n",
    "best_model = trained_models[best_model_name]\n",
    "best_results = training_results[best_model_name]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"   Selection Criterion: Highest F1 Score\")\n",
    "print(f\"   (F1 Score balances precision and recall - ideal for fraud detection)\")\n",
    "\n",
    "print(\"\\n\\nFinal Performance Metrics:\")\n",
    "print(\"-\"*40)\n",
    "print(f\"  Accuracy:  {best_results['accuracy']*100:.2f}%\")\n",
    "print(f\"  Precision: {best_results['precision']*100:.2f}%\")\n",
    "print(f\"  Recall:    {best_results['recall']*100:.2f}%\")\n",
    "print(f\"  F1 Score:  {best_results['f1']*100:.2f}%\")\n",
    "print(f\"  ROC-AUC:   {best_results['roc_auc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for best model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"DETAILED CLASSIFICATION REPORT - {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + classification_report(y_test, best_results['y_pred'], \n",
    "                                    target_names=['Legitimate', 'Fraudulent']))\n",
    "\n",
    "print(\"\\nMetric Definitions:\")\n",
    "print(\"-\"*40)\n",
    "print(\" Precision: Of all predicted frauds, what % are actually frauds?\")\n",
    "print(\" Recall: Of all actual frauds, what % did we catch?\")\n",
    "print(\" F1-Score: Harmonic mean of precision and recall\")\n",
    "print(\" Support: Number of actual occurrences of each class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final confusion matrix for best model\n",
    "cm = confusion_matrix(y_test, best_results['y_pred'])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create annotations\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "annotations = np.array([[f'TN\\n{cm[0,0]:,}\\n({cm_percent[0,0]:.1f}%)', \n",
    "                         f'FP\\n{cm[0,1]:,}\\n({cm_percent[0,1]:.1f}%)'],\n",
    "                        [f'FN\\n{cm[1,0]:,}\\n({cm_percent[1,0]:.1f}%)', \n",
    "                         f'TP\\n{cm[1,1]:,}\\n({cm_percent[1,1]:.1f}%)']])\n",
    "\n",
    "sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues',\n",
    "            xticklabels=['Predicted\\nLegitimate', 'Predicted\\nFraudulent'],\n",
    "            yticklabels=['Actual\\nLegitimate', 'Actual\\nFraudulent'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "\n",
    "plt.title(f'Final Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBusiness Impact Analysis:\")\n",
    "print(\"-\"*40)\n",
    "print(f\" Frauds Detected (TP): {tp:,} out of {tp+fn:,} ({tp/(tp+fn)*100:.2f}%)\")\n",
    "print(f\" Frauds Missed (FN): {fn:,} - These would result in financial losses\")\n",
    "print(f\" False Alarms (FP): {fp:,} - Legitimate transactions incorrectly blocked\")\n",
    "print(f\" Correct Approvals (TN): {tn:,} - Legitimate transactions correctly approved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"                    PROJECT SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "DATASET CHARACTERISTICS:\n",
    "──────────────────────────────────────────────────────────────────────\"\"\")\n",
    "print(f\"    Total Transactions: {len(df):,}\")\n",
    "print(f\"    Features: 29 (V1-V28 + Amount)\")\n",
    "print(f\"    Class Distribution: {'Balanced' if 0.8 <= class_percentages[1]/class_percentages[0] <= 1.2 else 'Imbalanced'}\")\n",
    "print(f\"    Legitimate: {class_counts[0]:,} ({class_percentages[0]:.2f}%)\")\n",
    "print(f\"    Fraudulent: {class_counts[1]:,} ({class_percentages[1]:.2f}%)\")\n",
    "\n",
    "print(\"\"\"\n",
    "PREPROCESSING STEPS:\n",
    "──────────────────────────────────────────────────────────────────────\"\"\")\n",
    "print(\"   1. Removed non-predictive 'id' column\")\n",
    "print(\"   2. Scaled 'Amount' feature using RobustScaler\")\n",
    "print(\"   3. Performed 80/20 stratified train-test split\")\n",
    "\n",
    "print(\"\"\"\n",
    "MODELS EVALUATED:\n",
    "──────────────────────────────────────────────────────────────────────\"\"\")\n",
    "print(\"    Logistic Regression (baseline)\")\n",
    "print(\"    Decision Tree\")\n",
    "print(\"    Random Forest\")\n",
    "print(\"    XGBoost\")\n",
    "print(\"    Isolation Forest (unsupervised, for comparison)\")\n",
    "\n",
    "print(\"\"\"\n",
    "MODEL PERFORMANCE SUMMARY:\n",
    "──────────────────────────────────────────────────────────────────────\"\"\")\n",
    "print(comparison_display.to_string(index=False))\n",
    "\n",
    "print(f\"\"\"\n",
    "🏆 BEST MODEL: {best_model_name}\n",
    "──────────────────────────────────────────────────────────────────────\n",
    "   Selection Criterion: Highest F1 Score\n",
    "   \n",
    "   Final Metrics:\n",
    "    Accuracy:  {best_results['accuracy']*100:.2f}%\n",
    "    Precision: {best_results['precision']*100:.2f}%\n",
    "    Recall:    {best_results['recall']*100:.2f}%\n",
    "    F1 Score:  {best_results['f1']*100:.2f}%\n",
    "    ROC-AUC:   {best_results['roc_auc']*100:.2f}%\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "KEY INSIGHTS:\n",
    "──────────────────────────────────────────────────────────────────────\"\"\")\n",
    "print(\"   1. TO BE DONE...\")\n",
    "print(\"   2. TO BE DONE...\")\n",
    "print(\"   3. TO BE DONE...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                         END OF ANALYSIS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Save the Best Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model for future use\n",
    "import joblib\n",
    "\n",
    "# Save model\n",
    "model_filename = f'fraud_detection_model_{best_model_name.lower().replace(\" \", \"_\")}.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "print(f\" Model saved as '{model_filename}'\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'amount_scaler.joblib')\n",
    "print(\" Scaler saved as 'amount_scaler.joblib'\")\n",
    "\n",
    "print(\"\\n--- How to load and use the model ---\")\n",
    "print(\"\"\"\n",
    "# Load model and scaler\n",
    "model = joblib.load('fraud_detection_model_xxx.joblib')\n",
    "scaler = joblib.load('amount_scaler.joblib')\n",
    "\n",
    "# Preprocess new data\n",
    "new_data['Amount'] = scaler.transform(new_data[['Amount']])\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_data)\n",
    "probabilities = model.predict_proba(new_data)[:, 1]\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
